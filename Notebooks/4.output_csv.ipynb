{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Base Evaluation-output CSV\n",
    "This notebook aims to examine the data status across all databases before initiating the data cleaning process. To understand which feature classes contain data or values within each database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import arcpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the feature class in all GDBs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the goal is to output the data status in each database:\n",
    "1. Feature Class: Identifying the feature class present in each database.\n",
    "2. Type: Specifying the type of the feature class.\n",
    "3. Feature Dataset: Easier identification of the location where the feature class is stored.\n",
    "4. Presented: Indicating if the geodatabase (GDB) has this feature class and has a value (\"1\"), if the GDB has the feature class but no value (\"0\"), and if the GDB doesn't have this feature class, the cell is left blank.\n",
    "5. Feature Count: Showing the number of data entries in each feature class.\n",
    "6. Percentage of Non-NULL Fields, Number of Non-NULL Values, Number of NULL Values, Number of Total Values, Number of Fields: These five columns provide insights into the distribution of values and non-values within each feature class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Original                    | Rename                              |\n",
    "|-----------------------------|-------------------------------------|\n",
    "| AB201_JAN2024               | manu_dayek_agadez_2024-01          |\n",
    "| Niamey_SDSFIE_311_Master_18MAY2023 | diori_hamani_niamey_2023-05   |\n",
    "| CIP_311_AB201               | manu_dayek_agadez_2022-04          |\n",
    "| Niamey_SDSFIE_311_Master_9Jan2023 | diori_hamani_niamey_2023-01   |\n",
    "| Chebelley_40_MASTER         | chabelley_2023-03                   |\n",
    "| SDSFIE_05Apr2023            | manda_bay_2023-04                   |\n",
    "| Chebelley_40_MASTER_May2024_small  | chabelley_2024-05            |\n",
    "| MandaBay_SDSFIE_40_MASTER_Small  | manda_bay_2024-05              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkGDB(gdb_path, output_csv_path):\n",
    "    arcpy.env.workspace = gdb_path\n",
    "    \n",
    "    # Extract the last part of the GDB path\n",
    "    gdb_basename = os.path.basename(gdb_path)\n",
    "\n",
    "    # Create a CSV file for output\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write GDB name row\n",
    "        csv_writer.writerow([ f\" {gdb_basename} \", \"\", \"\"])\n",
    "\n",
    "        # Write header to CSV\n",
    "        csv_writer.writerow([\"Feature Class\", \"Presented\", \"Feature Count\"])\n",
    "        \n",
    "        # Check if Feature Datasets exist\n",
    "        feature_datasets = arcpy.ListDatasets(\"\", \"\")\n",
    "        if feature_datasets:\n",
    "            # Loop through FDs\n",
    "            print(f\"--- GDB {gdb_basename} ---\")  # Print GDB name once\n",
    "            for fd in feature_datasets:\n",
    "                arcpy.env.workspace = os.path.join(gdb_path, fd)\n",
    "                feature_classes = arcpy.ListFeatureClasses()\n",
    "\n",
    "                print(f\"--- {fd} ---\")  # Print FD name\n",
    "\n",
    "                for fc in feature_classes:\n",
    "                    describe_and_calculate(fc, csv_writer, fd)\n",
    "        else:\n",
    "            # No Feature Datasets, directly process Feature Classes\n",
    "            feature_classes = arcpy.ListFeatureClasses()\n",
    "            for fc in feature_classes:\n",
    "                describe_and_calculate(fc, csv_writer, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_and_calculate(fc, csv_writer, fd):\n",
    "    description = arcpy.Describe(fc)\n",
    "    count_result = arcpy.management.GetCount(fc)\n",
    "    feature_count = int(count_result.getOutput(0))\n",
    "    \n",
    "    if feature_count == 0:\n",
    "        feature_status = \"0\"\n",
    "    else:\n",
    "        feature_status = \"1\"\n",
    "\n",
    "    total_fields = 0\n",
    "    non_null_fields = 0\n",
    "    null_fields = 0\n",
    "    percentage_with_values = \"\"\n",
    "\n",
    "    # Obtain the fields with value\n",
    "    fields = arcpy.ListFields(fc)\n",
    "\n",
    "    for field in fields:\n",
    "        with arcpy.da.SearchCursor(fc, [field.name]) as cursor:\n",
    "            for row in cursor:\n",
    "                for value in row:\n",
    "                    total_fields += 1\n",
    "                    if value is not None:\n",
    "                        non_null_fields += 1\n",
    "                    else:\n",
    "                        null_fields += 1\n",
    "    \n",
    "    # Calculate the percentage of each attribute\n",
    "    if total_fields > 0:\n",
    "        percentage_with_values = (non_null_fields / total_fields) * 100\n",
    "\n",
    "    # Write results to CSV\n",
    "    csv_writer.writerow([fc, feature_status, feature_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_fd_paths = [\n",
    "    # previous\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\02_raw_no_duplicates_or_empty_feature_layers\\chabelley_2023-03.gdb\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\02_raw_no_duplicates_or_empty_feature_layers\\manda_bay_2023-04.gdb\",\n",
    "    \n",
    "    # updated gdb Jul2024\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\02_raw_no_duplicates_or_empty_feature_layers\\chabelley_2024-05.gdb\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\02_raw_no_duplicates_or_empty_feature_layers\\manda_bay_2024-05.gdb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty DataFrame before the loop\n",
    "\n",
    "for gdb_path in list_of_fd_paths:\n",
    "    # Extract GDB filename without extension\n",
    "    gdb_name = os.path.splitext(os.path.basename(gdb_path))[0]\n",
    "    \n",
    "    # Construct CSV filename using GDB name\n",
    "    output_csv_path = rf\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\{gdb_name}.csv\"\n",
    "    \n",
    "    checkGDB(gdb_path, output_csv_path)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge by Feature Classes\n",
    "merge all the GDB's evaluation CSVs together with same feature class, so that we can have the insight of cross base evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_csv_paths = [\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\chabelley_2023-03.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\chabelley_2024-05.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\manda_bay_2023-04.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\manda_bay_2024-05.csv\"\n",
    "]\n",
    "\n",
    "# Create an initial DataFrame with the Feature Class columns\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for csv_path in list_of_csv_paths:\n",
    "    gdb_basename = os.path.splitext(os.path.basename(csv_path))[0] # Extracting basename without extension\n",
    "    \n",
    "\n",
    "\n",
    "    # Read CSV file, skip the first row, and use the second row as header\n",
    "    current_df = pd.read_csv(csv_path, skiprows=1)\n",
    "    # Select only the desired columns\n",
    "\n",
    "    current_df = current_df[[\"Feature Class\", \"Presented\", \"Feature Count\"]]\n",
    "    # Rename columns with suffixes\n",
    "    current_df.columns = [\"Feature Class\", f\"Presented_{gdb_basename}\", f\"Feature Count_{gdb_basename}\"]\n",
    "\n",
    "    # Convert 'Feature Class' values to lowercase\n",
    "    current_df['Feature Class'] = current_df['Feature Class'].str.lower()\n",
    "    # Merge DataFrames with custom suffixes\n",
    "    if merged_df.empty:\n",
    "        merged_df = current_df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, current_df, on='Feature Class', how='outer')\n",
    "\n",
    "# Specify the output path for the merged CSV file\n",
    "output_merged_csv_path = r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all_updated.csv\"\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_merged_csv_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13]:31: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Feature Class_x'} in the result is deprecated and will raise a MergeError in a future version.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "list_of_csv_paths = [\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\chabelley_2023-03.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\chabelley_2024-05.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\manda_bay_2023-04.csv\",\n",
    "    r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\manda_bay_2024-05.csv\"\n",
    "]\n",
    "\n",
    "# Create an initial DataFrame with the Feature Class columns\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for csv_path in list_of_csv_paths:\n",
    "    gdb_basename = os.path.splitext(os.path.basename(csv_path))[0] # Extracting basename without extension\n",
    "\n",
    "    # Read CSV file, skip the first row, and use the second row as header\n",
    "    current_df = pd.read_csv(csv_path, skiprows=1)\n",
    "    # Select only the desired columns\n",
    "    current_df = current_df[[\"Feature Class\", \"Presented\", \"Feature Count\"]]\n",
    "    # Rename columns with suffixes\n",
    "    current_df.columns = [\"Feature Class\", f\"Presented_{gdb_basename}\", f\"Feature Count_{gdb_basename}\"]\n",
    "\n",
    "    # Create a lowercase version of 'Feature Class' for merging\n",
    "    current_df['Feature Class_lower'] = current_df['Feature Class'].str.lower()\n",
    "\n",
    "    # Merge DataFrames with custom suffixes\n",
    "    if merged_df.empty:\n",
    "        merged_df = current_df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, current_df, on='Feature Class_lower', how='outer')\n",
    "\n",
    "# Drop the auxiliary lowercase column after merging\n",
    "merged_df.drop(columns=['Feature Class_lower'], inplace=True)\n",
    "\n",
    "# Specify the output path for the merged CSV file\n",
    "output_merged_csv_path = r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all_updated.csv\"\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_merged_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this initial assessment, we can delve deeper into the data by utilizing CSV files. This will provide insights into the quantity of data with values, guiding our approach for continued data cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
