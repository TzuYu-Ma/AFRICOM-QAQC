{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Base Evaluation\n",
    "This notebook aims to examine the data status across all databases before initiating the data cleaning process. To understand which feature classes contain data or values within each database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import arcpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the feature class in all GDBs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the goal is to output the data status in each database:\n",
    "1. Feature Class: Identifying the feature class present in each database.\n",
    "2. Type: Specifying the type of the feature class.\n",
    "3. Feature Dataset: Easier identification of the location where the feature class is stored.\n",
    "4. Presented: Indicating if the geodatabase (GDB) has this feature class and has a value (\"1\"), if the GDB has the feature class but no value (\"0\"), and if the GDB doesn't have this feature class, the cell is left blank.\n",
    "5. Feature Count: Showing the number of data entries in each feature class.\n",
    "6. Percentage of Non-NULL Fields, Number of Non-NULL Values, Number of NULL Values, Number of Total Values, Number of Fields: These five columns provide insights into the distribution of values and non-values within each feature class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkGDB(gdb_path, output_csv_path):\n",
    "    arcpy.env.workspace = gdb_path\n",
    "    \n",
    "    # Extract the last part of the GDB path\n",
    "    gdb_basename = os.path.basename(gdb_path)\n",
    "\n",
    "    # Create a CSV file for output\n",
    "    with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write GDB name row\n",
    "        csv_writer.writerow([\"\", \"\", \"\", f\" {gdb_basename} \", \"\", \"\", \"\", \"\"])\n",
    "\n",
    "        # Write header to CSV\n",
    "        csv_writer.writerow([\"Feature Class\", \"Type\", \"Feature Dataset\", \"Presented\", \"Feature Count\",\n",
    "                             \"Percentage of Non-NULL Fields\",\"Number of Non-NULL Values\", \"Number of NULL Values\",\n",
    "                             \"Number of Total Values\", \"Number of fields\"])\n",
    "        \n",
    "        # Check if Feature Datasets exist\n",
    "        feature_datasets = arcpy.ListDatasets(\"\", \"\")\n",
    "        if feature_datasets:\n",
    "            # Loop through FDs\n",
    "            print(f\"--- GDB {gdb_basename} ---\")  # Print GDB name once\n",
    "            for fd in feature_datasets:\n",
    "                arcpy.env.workspace = os.path.join(gdb_path, fd)\n",
    "                feature_classes = arcpy.ListFeatureClasses()\n",
    "\n",
    "                print(f\"--- {fd} ---\")  # Print FD name\n",
    "\n",
    "                for fc in feature_classes:\n",
    "                    describe_and_calculate(fc, csv_writer, fd)\n",
    "        else:\n",
    "            # No Feature Datasets, directly process Feature Classes\n",
    "            feature_classes = arcpy.ListFeatureClasses()\n",
    "            for fc in feature_classes:\n",
    "                describe_and_calculate(fc, csv_writer, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_and_calculate(fc, csv_writer, fd):\n",
    "    description = arcpy.Describe(fc)\n",
    "    count_result = arcpy.management.GetCount(fc)\n",
    "    feature_count = int(count_result.getOutput(0))\n",
    "    \n",
    "    if feature_count == 0:\n",
    "        feature_status = \"0\"\n",
    "    else:\n",
    "        feature_status = \"1\"\n",
    "\n",
    "    total_fields = 0\n",
    "    non_null_fields = 0\n",
    "    null_fields = 0\n",
    "    percentage_with_values = \"\"\n",
    "\n",
    "    # Obtain the fields with value\n",
    "    fields = arcpy.ListFields(fc)\n",
    "\n",
    "    for field in fields:\n",
    "        with arcpy.da.SearchCursor(fc, [field.name]) as cursor:\n",
    "            for row in cursor:\n",
    "                for value in row:\n",
    "                    total_fields += 1\n",
    "                    if value is not None:\n",
    "                        non_null_fields += 1\n",
    "                    else:\n",
    "                        null_fields += 1\n",
    "    \n",
    "    # Calculate the percentage of each attribute\n",
    "    if total_fields > 0:\n",
    "        percentage_with_values = (non_null_fields / total_fields) * 100\n",
    "\n",
    "    # Write results to CSV\n",
    "    csv_writer.writerow([fc, description.shapeType, fd, feature_status, feature_count, percentage_with_values, \n",
    "                         non_null_fields, null_fields, non_null_fields + null_fields, len(fields)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_fd_paths = [\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\Agadez_SDSFIE_MASTER_2022April\\Agadez_Master_2022April\\CIP_311_AB201.gdb\",\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\Chabelley_SDSFIE_MASTER_March2023\\Chebelley_40_MASTER.gdb\",\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\Chabelley_SDSFIE_MASTER_March2023\\Chabelley_SDSFIE_MASTER_March2023.gdb\",\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\Chabelley_SDSFIE_MASTER_March2023\\Chebelly_40_MASTER_CIP.gdb\",\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\Niamey_SDSFIE_311_Master_9Jan2023\\Niamey_SDSFIE_311_Master_9Jan2023.gdb\",\n",
    "    #r\"D:\\spring2024\\AFRICOM\\DoD SAFE-Z6ZsX3Em6bkXVeBq\\CIP_Africa_Data_2023\\SDSFIE_MandaBay_MASTER_2023\\SDSFIE_05Apr2023.gdb\"\n",
    "    # updated gdb\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\AB201_JAN2024.gdb',\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\Niamey_SDSFIE_311_Master_18MAY2023.gdb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty DataFrame before the loop\n",
    "\n",
    "for gdb_path in list_of_fd_paths:\n",
    "    # Extract GDB filename without extension\n",
    "    gdb_name = os.path.splitext(os.path.basename(gdb_path))[0]\n",
    "    \n",
    "    # Construct CSV filename using GDB name\n",
    "    #output_csv_path = rf\"D:\\spring2024\\cross_base_evaluation\\{gdb_name}.csv\"\n",
    "    output_csv_path = rf\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\{gdb_name}.csv\"\n",
    "    \n",
    "    checkGDB(gdb_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge by Feature Classes\n",
    "merge all the GDB's evaluation CSVs together with same feature class, so that we can have the insight of cross base evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "In  \u001b[0;34m[9]\u001b[0m:\nLine \u001b[0;34m15\u001b[0m:    merged_df = pd.DataFrame(columns=[\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].str.lower())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'str'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "list_of_csv_paths = [\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\Chebelley_40_MASTER.csv\",\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\Chebelly_40_MASTER_CIP.csv\",\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\Chabelley_SDSFIE_MASTER_March2023.csv\",\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\CIP_311_AB201.csv\",\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\Niamey_SDSFIE_311_Master_9Jan2023.csv\",\n",
    "    #r\"D:\\spring2024\\cross_base_evaluation\\SDSFIE_05Apr2023.csv\"\n",
    "    # updated csv\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\AB201_JAN2024.csv',\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\Niamey_SDSFIE_311_Master_18MAY2023.csv',\n",
    "]\n",
    "\n",
    "\n",
    "# Create an initial DataFrame with the Feature Class columns\n",
    "merged_df = pd.DataFrame(columns=[\"Feature Class\"].str.lower())\n",
    "\n",
    "for csv_path in list_of_csv_paths:\n",
    "    gdb_basename = os.path.basename(csv_path)\n",
    "\n",
    "    # Read CSV file, skip the first row, and use the second row as header\n",
    "    current_df = pd.read_csv(csv_path, skiprows=1)\n",
    "\n",
    "    # Select only the desired columns\n",
    "    current_df = current_df[[\"Feature Class\", \"Presented\", \"Feature Count\"]]\n",
    "\n",
    "    print(current_df)\n",
    "    # Rename columns with suffixes\n",
    "    current_df.columns = [\"Feature Class\", f\"Presented_{gdb_basename}\", f\"Feature Count_{gdb_basename}\"]\n",
    "\n",
    "\n",
    "    # Merge DataFrames with custom suffixes\n",
    "    merged_df = pd.merge(merged_df, current_df, on=['Feature Class'], how='outer')\n",
    "\n",
    "# Specify the output path for the merged CSV file\n",
    "#output_merged_csv_path = r\"D:\\spring2024\\cross_base_evaluation\\cross_base_evaluation_all.csv\"\n",
    "output_merged_csv_path = r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all_updated.csv\"\n",
    "\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_merged_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_csv_paths = [\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\AB201_JAN2024.csv',\n",
    "    r'C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\Niamey_SDSFIE_311_Master_18MAY2023.csv',\n",
    "]\n",
    "\n",
    "# Create an initial DataFrame with the Feature Class columns\n",
    "merged_df = pd.DataFrame(columns=[\"Feature Class\"])\n",
    "\n",
    "for csv_path in list_of_csv_paths:\n",
    "    gdb_basename = os.path.basename(csv_path)\n",
    "\n",
    "    # Read CSV file, skip the first row, and use the second row as header\n",
    "    current_df = pd.read_csv(csv_path, skiprows=1)\n",
    "\n",
    "    # Select only the desired columns\n",
    "    current_df = current_df[[\"Feature Class\", \"Presented\", \"Feature Count\"]]\n",
    "\n",
    "    # Rename columns with suffixes\n",
    "    current_df.columns = [\"Feature Class\", f\"Presented_{gdb_basename}\", f\"Feature Count_{gdb_basename}\"]\n",
    "\n",
    "    # Convert 'Feature Class' values to lowercase\n",
    "    current_df['Feature Class'] = current_df['Feature Class'].str.lower()\n",
    "\n",
    "    # Merge DataFrames with custom suffixes\n",
    "    merged_df = pd.merge(merged_df, current_df, on='Feature Class', how='outer')\n",
    "\n",
    "# Specify the output path for the merged CSV file\n",
    "output_merged_csv_path = r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all_updated.csv\"\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_merged_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first DataFrame from the updated CSV file\n",
    "df1 = pd.read_csv(r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all_updated.csv\")\n",
    "\n",
    "# Read the second DataFrame from the original CSV file\n",
    "df2 = pd.read_csv(r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\cross_base_evaluation_all - cross_base_evaluation_all.csv\")\n",
    "\n",
    "# Convert 'Feature Class' values to lowercase in both DataFrames\n",
    "df1['Feature Class'] = df1['Feature Class'].str.lower()\n",
    "df2['Feature Class'] = df2['Feature Class'].str.lower()\n",
    "\n",
    "# Merge the two DataFrames on the lowercase version of the 'Feature Class' column using an outer join\n",
    "df3 = pd.merge(df1, df2, on='Feature Class')\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "df3.to_csv(r\"C:\\Users\\ma000551\\Desktop\\AFRICOM\\AFRICOM_CLEAN_DATA\\04_update\\updated_cross_base_evaluation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this initial assessment, we can delve deeper into the data by utilizing CSV files. This will provide insights into the quantity of data with values, guiding our approach for continued data cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
